

Statistics such as the (Root Mean Square Error) RMSE, are commonly used when simulation testing assessment models \citep[e.g.][]{horbowy2011,kell2016xval}. RMSE, however, does not describe average error alone, favours forecasts that avoid large deviations from the mean, and cannot be used to compare across series. A more robust and easier to interpret statistic for evaluating prediction skill is the Mean Absolute Scaled Error (MASE) \citep{Hyndman2006}. MASE evaluates a model’s prediction skill relative to a na\ddot{i}ve baseline prediction. A prediction is said to have skill if it improves the model forecast compared to the baseline. A widely used baseline forecast for time series is the persistence algorithm that simply takes the value at the previous time step to predict the expected outcome at the next time step as a na\ddot{i}ve in-sample prediction, i.e. tomorrow will the same as today 

The MASE score scales the mean absolute error of the forecasts by the mean absolute error of a na\ddot{i}ve in-sample prediction, such that:

If $Y_t$ is the variable of interest at time $t$ and ${\hat{Y}_t}$ is it's predicted value then the prediction error is $e_t = Y_t - \hat{Y}_t$. For a series of $T$ observations and predictions

\begin{equation} {MASE={\frac{\sum _{t=1}^{T}\left|e_{t}\right|}{\frac {T}{T-1}}\sum _{t=1}^{T}\left|Y_{t+1}-Y_{t}\right|}} \end{equation}

The MASE has the desirable properties of scale invariance, predictable behaviour, symmetry, interpretability and asymptotic normality. MASE is independent of the scale of the data, so can be used to compare forecasts across data sets with different scales. Behaviour is predictable as $y_{t}\rightarrow 0$] Percentage forecast accuracy measures such as the Mean absolute percentage error (MAPE) rely on division of $y_{t}$, skewing the distribution of the MAPE for values of $y_{t}$ near or equal to 0. This is especially problematic for data sets whose scales do not have a meaningful 0, such as temperature in Celsius or Fahrenheit, and for intermittent demand data sets, where $y_{t}=0$  occurs frequently.

Symmetry since The mean absolute scaled error penalises positive and negative forecast errors equally, and penalises errors in large forecasts and small forecasts equally. MASE can be easily interpreted, as values greater than one indicate that in-sample one-step forecasts from the na\ddot{i}ve method perform better than the forecast values under consideration, in other words model forecasts are worse than a random walk. Conversely, a MASE score of 0.5 indicates that the model forecasts twice as accurate as a na\ddot{i}ve baseline prediction; thus the model has prediction skill.  


If $Y_t$ is a variable of interest at time $t$ and \eqn{\hat{Y}_t} is it's predicted value then the prediction error is given by $e_t = Y_t - \hat{Y}_t$. For a series of $T$ observations and predictions The accuracy of the predictions can be compared to the actual value by calculating various measures, such as as the Mean Absolute Error (MAE), which is the mean of the absolute errors and tells us how big of an error we can expect from the forecast on average. \\

\eqn{MAE=\frac{\left|e_t\right|}{T}} \\

A problem with the MAE is that the relative size of the error is not always obvious. Sometimes it is hard to tell a big error from a small error. To deal with this problem, we can compute the MAPE instead, i.e. MAE as a percentage, this allows forecasts of different series in different scales to be compared.\\

\eqn{MAPE = \frac{1}{T} \sum_{t=1}^T 100\, \left|\frac{e_t}{Y_t}\right|}\\

Both MAE and MAPE are based on the mean error and so may understate the impact of big, but infrequent, errors. If we focus too much on the mean, we will be caught off guard by the infrequent big error. To adjust for large rare errors, we calculate the Root Mean Square Error (RMSE). By squaring the errors before we calculate their mean and then taking the square root of the mean, we arrive at a measure of the size of the error that gives more weight to the large but infrequent errors than the mean.\\

\eqn{RMSE = \sqrt{\frac{1}{T} \sum_{t=1}^T e_t^2}} \\

We can also compare RMSE and MAE to determine whether the forecast contains large but infrequent errors. The larger the difference between RMSE and MAE the more inconsistent the error size.
 
Another measure is the Mean Absolute Scaled Error (MASE) \\

\eqn{MASE={\frac {\sum _{t=1}^{T}\left|e_{t}\right|}{{\frac {T}{T-1}}\sum _{t=1}^{T}\left|Y_{t+1}-Y_{t}\right|}}} \\

Which has the desirable properties of scale invariance, predictable behaviour, symmetry, interpretability and asymptotic normality
 
The mean absolute scaled error is independent of the scale of the data, so can be used to compare forecasts across data sets with different scales. Behaviour is predictable as $y_{t}\rightarrow 0$] Percentage forecast accuracy measures such as the Mean absolute percentage error (MAPE) rely on division of $y_{t}$, skewing the distribution of the MAPE for values of $y_{t}$ near or equal to 0. This is especially problematic for data sets whose scales do not have a meaningful 0, such as temperature in Celsius or Fahrenheit, and for intermittent demand data sets, where $y_{t}=0$  occurs frequently.

Symmetry since The mean absolute scaled error penalises positive and negative forecast errors equally, and penalises errors in large forecasts and small forecasts equally. In contrast, the MAPE  fail both of these criteria. The mean absolute scaled error can be easily interpreted, as values greater than one indicate that in-sample one-step forecasts from the naïve method perform better than the forecast values under consideration.The Diebold-Mariano test for one-step forecasts is used to test the statistical significance of the difference between two sets of forecasts. To perform hypothesis testing with the Diebold-Mariano test statistic, it is desirable for $DM ∼ N ( 0 , 1 )$ $DM\sim N(0,1)$ , where $DM$ is the value of the test statistic. The DM statistic for the MASE has been empirically shown to approximate this distribution, while the mean relative absolute error (MRAE), MAPE and sMAPE do not.
 
Another measure is Theil's $U$\\
  
\eqn{U= \sqrt{\frac{1}{T}\\
     \sum_{t=1}^{T-1} \left(\frac{e_{t+1}}{Y_t}\right)^2
     \cdot \left[
    \frac{1}{T} \sum_{t=1}^{T-1} 
        \left(\frac{Y_{t+1} - Y_t}{Y_t}\right)^2 \right]^{-1}}}

The more accurate the forecasts, the lower the value of Theil's $U$,   which has a minimum of 0. This measure can be interpreted as the ratio of the RMSE of the proposed forecasting model to the RMSE of  a na\"ive model which simply predicts $Y_{t+1} = Y_t$ for all $t$. The na\"ive model yields $U = 1$; values less than 1 indicate an  improvement relative to this benchmark and values greater than 1 a deterioration.

Altough the methods have their limitations, they are simple tools for evaluating forecast accuracy that can be used without knowing anything about the forecast except the past values of a forecast.

Just because a forecast has been accurate in the past, however, does not mean it will be accurate in the future. Over fitting may make the forecast less accurate and there is always the possibility of an event occurring that the model cannot anticipate, a black swan event. When this happens, you don’t know how big the error will be. Errors associated with these events are not typical errors, which is what the statistics above try to measure. So, while forecast accuracy can tell us a lot about the past, remember these limitations when using forecasts to predict the future.
