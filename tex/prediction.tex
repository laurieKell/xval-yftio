
\section{Prediction Skill}

Prediction skill
The model diagnostics introduced thus far evaluated how well the model fits historical observations. To provide fisheries management advice, however, requires predicting the response of a stock to management and checking that predictions are consistent with reality (Kell et al. 2016). The accuracy and precision of the predictions depend on the validity of the model, the information in the data, and how far ahead we wish to predict (i.e. the prediction horizon). 
Retrospective analysis  (Hurtado-Ferro et al., 2014, Carvalho et al. 2017) is commonly used to check the consistency of model estimates, i.e. SSB and fishing mortality. Retrospective analysis is a hindcasting approach that involves sequentially removing  observations from the terminal year (peels), fitting the model to the truncated series and then comparing the relative difference between model estimates from full time series with the truncated time-series. Retrospective analysis focusses on the bias and accuracy of modelled quantitates. The most commonly used statistic is Mohn’s (1999) rho (). In line with recent studies (Carvalho et al. 2016; Winker et al. 2018), we focus on the formulation proposed by Hurtado-Ferro et al. (2014): 

					(2)

where  is quantity for which Mohn’s is being calculated,  is the corresponding estimate from the reference model that was fitted to the full dataset, T is the terminal year of the assessment and h denotes the number hindcasted time steps. While it is fairly straight forward to compare the  statistic among alternative model runs, the decisions of whether the Mohn’s rho statistic of the ‘best’ model is acceptable or not can be to some extent subjective. To address this, a “rule of thumb” was proposed by Hurtado-Ferro et al. (2014), suggesting values of Mohn’s rho that fall outside the range (-0.15 to 0.20) can be interpreted as an indication of an undesirable retrospective pattern for e.g. longer lived species.
Validation is an important prequist in many fields, e.g. in energy and climate models as validation increases confidence in the outputs of a model and leads to an increase in trust amongst the public and policy makers (Kell at al., submitted). It is not possible to validate a model, however, using derived quantities, such as SSB and F, since for a model to be valid four prerequisites must be satisfied (Hodge and Dewar, 1992), namely the situtaion i) must observable and measurable. ii) exhibit constancy of structure in time, iii) exhibit constancy across variations in conditions not specified in the model and it iv) must be possible to collect sufficient data. Validation examines if a model should be modified or extended, and is complementary to hypothesis testing which examines if the model structure can be reduced. Therefore Kell et al. (2016) proposed a model-free hindcasting technique (HCXval) using crossvalidation where observations (e.g. CPUE) are compared to their predicted future values. The key concept behind the HCXval approach is ‘prediction skill’, which is defined as any measure of accuracy of a forecasted value ( to the actual observed value that is  not known by the model (Huschke, 1959), where the difference  is hereafter referred to as ‘prediction residual’. The HCXval algorithm is similar to that used in retrospective analysis as it requires the same two routine procedures of peeling the observations and re-fitting the model to the truncated data series. The key differences to retrospective analysis are that HCXval involves the additional steps of projecting over the missing years and then cross-validating these forecasts against observations to assess the model’s prediction skill.
Statistics such as the (Root Mean Square Error) RMSE, are commonly used when simulation testing assessment models (e.g. Horbowy, 2011; Kell et al. 2016). RMSE, however, does not describe average error alone, favours forecasts that avoid large deviations from the mean, and cannot be used to compare across series. An easier to interpret and more robust statistic for evaluating prediction skill is the Mean Absolute Scaled Error (MASE) proposed by Hyndman and Koehler (2006). MASE builds on the principle of evaluating a model’s prediction skill relative to a naïve baseline prediction, e.g. tomorrow will the same as today. Where a ‘prediction’ is said to have ‘skill’ if it improves the model forecast compared to the baseline. A widely used baseline forecast for time series is the ‘persistence algorithm’ that simply takes the value at the previous time step to predict the expected outcome at the next time step as a naïve in-sample prediction. The MASE score scales the mean absolute error of forecasts (prediction residuals) to the mean absolute error of a naïve in-sample prediction, such that:
									(3)

where  is the one-step ahead forecast of the expected value for the observation at time t and h denotes the number of hindcasting time steps for which forecasts   were made. A MASE score greater than one can then be interpreted such that the average model forecasts are worse, than a random walk. Conversely, a MASE score of 0.5 indicates that the model forecasts twice as accurate as a naïve baseline prediction; thus the model has prediction skill.  

Predictive skill [Part II]: COOKBOOK 
Retrospective analysis was implemented in Stock Synthesis utilizing R (R Core Team 2018) and the function `SS_doRetro()` and `SSmohnsrho()` available in the `r4ss` package (Taylor et al. 2018). The `SS_doRetro()` function sequentially implements the “retrospective year” option in the Stock Synthesis “starter.ss” file (e.g., as described the Stock Synthesis manual for version 3.30.14; Methot et al. 2019) as follows: In step 1, a converged Stock Synthesis base model is identified, and a retrospective period is defined. In step 2, the `SS_doRetro()` function is run on the base model. Given this setup, `r4ss` generates a separate Stock Synthesis model run for the full model and for each retrospective year with data in the retrospective year removed from the model likelihood, and then runs each Stock Synthesis model to re-estimate all model parameters. In step 3, the model runs are then compiled and the results are summarized using the `r4ss` functions `SSgetoutput()` and `SSsummarize()`, respectively.  In step 4, the quantities of interest are plotted using relevant options in the `r4ss` function ` SSplotComparisons()`. The retrospective diagnostic was implemented here for the SFM stock assessments by sequentially eliminating the five most recent years of data from the full stock assessment model (a 5 year “peel”). In addition, the Mohn’s rho statistic (Hurtado-Ferro et al. 2014) was calculate  for ending year spawning stock size obtained from each peel relative to that obtained from the full model using the function `SSmohnsrho()` available in the r4ss package (Taylor et al. 2018). 
Results from the retrospective analysis are shown in Figure 8. There was a consistent pattern of change in the 5-year peel relative to the full stock assessment model. However, the Mohn’s rho statistic for spawning stock size was 0.062, which was between -0.15 and 0.20 and, consequently, indicated that the retrospective bias was relatively small (Hurtado-Ferro et al. 2014; Carvalho et al. 2017). The ending values of all peels also fell within the asymptotic 95% confidence interval of the full stock assessment model, which also indicated that the retrospective pattern was within the asymptotic 95% margin of error obtained from the full stock assessment model. 
Implementing the HCxval diagnostic in SS3 requires the same steps 1-3 as described above for the retrospective analysis. Conveniently, the `r4ss` function `SS_doRetro()` not only refits the models to curtailed data series, but also automatically projects forward until terminal year of the full reference model run given the observed catches. Therefore, there are no additional computationally intense tasks needed for HCxval if conducted in conjunction with a retrospective analysis. To implement HCxval for SS3, we developed a new function R function ‘SSplotHCxval()’, which requires the output object from `SSsummarize()` as input to produce a novel HCxval diagnostic plot and compute the MASE scores for each CPUE indices with observations falling within hindcasting period (i.e. here 2011-2015) as follows: In step 4, the practiced CPUE is computed as the product of the fleet-specific valuable  biomass trajectories and the estimated catchability coefficients q for the full model and for each retrospective model runs. In step 5, the prediction residuals are computed as the difference between the one-step ahead CPUE forecasts (peel + 1 for each retrospective model) and the corresponding CPUE observation. In step 6, the MASE score is calculated (Eq. 3) for each CPUE index by scaling the mean absolute error of the prediction residuals to the mean absolute error of baseline forecasts of CPUE observations from previous time step over the hindcasting evaluation period (2011-2015). 
Results from the HCxval are shown for the six CPUE indices that were fitted in SFM assessment model (Figure 9). In the case of SFM, the all CPUE indices included observation the fell within hindcasting evaluation period 2011-2015, which are highlighted as colour-coded solid circles with associated light-grey shaded 95% confidence interval. The expected CPUE is depicted by colour-coded solid lines, with the end points representing an one step ahead and the colour-codes matching the corresponding CPUE observation (i.e. year of peel + 1)

James S. Hodges and James A. Dewar. 1992. Is it you or your model talking? A framework for Model Validation. (1992).
Kell, A.J.M, Foreshaw, M., and McGough, S.A., Submitted. Long-Term Electricity Market Agent Based Model Validation using Genetic Algorithm based Optimization. e-Energy ’20, June 22–26, 2020, Melbourne, Australia


